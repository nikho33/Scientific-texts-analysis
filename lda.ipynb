{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://PC-NLA:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Scientific papers analysis app</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x146e1ff93c8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local = \"local[*]\"\n",
    "\n",
    "appName = \"Scientific papers analysis app\"\n",
    "\n",
    "configLocale = SparkConf().setAppName(appName).setMaster(local). \\\n",
    "set(\"spark.executor.memory\", \"4G\"). \\\n",
    "set(\"spark.driver.memory\", \"4G\"). \\\n",
    "set(\"spark.sql.catalogImplementation\", \"in-memory\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=configLocale).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id :  local-1565094004468\n",
      "Version :  2.4.3\n"
     ]
    }
   ],
   "source": [
    "print(\"Id : \", sc.applicationId)\n",
    "print(\"Version : \", sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@connectwithghosh/topic-modelling-with-latent-dirichlet-allocation-lda-in-pyspark-2cb3ebd5678e\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For building the model\n",
    "from pyspark.ml.feature import CountVectorizer, HashingTF, IDF\n",
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "#from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.ml.clustering import LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------+----+--------------------+--------------------+--------------------+\n",
      "|  filename|               title|      author|date|            abstract|            keywords|                text|\n",
      "+----------+--------------------+------------+----+--------------------+--------------------+--------------------+\n",
      "|PG0001.pdf|ai based personal...|chih-ming ho|2018|['cancer', 'patie...|['personalized', ...|['current', 'drug...|\n",
      "+----------+--------------------+------------+----+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"./Data/DataMicroTAS/\"\n",
    "filename = \"Datas_MicroTAS2018.csv\"\n",
    "# Reading the data\n",
    "data = sqlContext.read.format(\"csv\")\\\n",
    "   .options(header='true', inferschema='true') \\\n",
    "   .load(os.path.realpath(path + filename))\n",
    "\n",
    "#print(type(data), dir(data))\n",
    "data.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|  filename|                text|\n",
      "+----------+--------------------+\n",
      "|PG0001.pdf|[current, drug, d...|\n",
      "+----------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[filename: string, text: array<string>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selection = \"text\"\n",
    "data2 = data.select(\"filename\", F.regexp_replace(F.col(selection), \"[\\[\\]']\", \"\").alias(selection)) # [\\$#,]\n",
    "data2 = data2.select(\"filename\", split(col(selection), \",\\s*\").alias(selection))\n",
    "data2.show(1)\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokens = data.select(\"filename\", \"text\")\n",
    "#tokens.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. LDA algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|  filename|                text|\n",
      "+----------+--------------------+\n",
      "|PG0001.pdf|[current, drug, d...|\n",
      "|PG0004.pdf|[inerti, microflu...|\n",
      "+----------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[filename: string, text: array<string>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_txts = sqlContext.createDataFrame(tokens, [\"list_of_words\",'index'])\n",
    "df_txts = data2.select(\"filename\", \"text\")\n",
    "df_txts.show(2)\n",
    "df_txts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        raw_features|\n",
      "+--------------------+\n",
      "|(8000,[0,1,2,4,7,...|\n",
      "|(8000,[1,2,3,4,5,...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/vsmolyakov/pyspark/blob/master/lda.py\n",
    "# TF\n",
    "num_features = 8000 \n",
    "cv = CountVectorizer(inputCol=\"text\", outputCol=\"raw_features\", vocabSize=num_features, minDF=2.0)\n",
    "cvmodel = cv.fit(df_txts)\n",
    "\n",
    "vocab = cvmodel.vocabulary\n",
    "\n",
    "result_cv = cvmodel.transform(df_txts)\n",
    "#result_cv = result_cv.drop('text')\n",
    "result_cv.select(\"raw_features\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hashting TF\n",
    "# TF: Both HashingTF and CountVectorizer can be used to generate the term frequency vectors.\n",
    "# HashingTF is a Transformer which takes sets of terms and converts those sets into fixed-length feature vectors.\n",
    "# https://spark.apache.org/docs/2.2.0/ml-features.html\n",
    "\n",
    "#hashingTF = HashingTF(inputCol=\"raw_features\", outputCol=\"tf_features\", numFeatures=num_features)\n",
    "#result_hashing = hashingTF.transform(result_cv)\n",
    "#result_hashing = newsgroups.drop('raw_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        raw_features|\n",
      "+--------------------+\n",
      "|(8000,[0,1,2,4,7,...|\n",
      "|(8000,[1,2,3,4,5,...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# IDF\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idfModel = idf.fit(result_cv)\n",
    "result_tfidf = idfModel.transform(result_cv) \n",
    "#result_tfidf = result_tfidf.drop('raw_features')\n",
    "result_cv.select(\"raw_features\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "num_topics = 10\n",
    "\n",
    "lda = LDA(k=num_topics, featuresCol=\"features\", seed=0)\n",
    "model = lda.fit(result_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|topic|         termIndices|         termWeights|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[279, 324, 1273, ...|[0.05616231648378...|\n",
      "|    1|[298, 3550, 130, ...|[0.01853075520755...|\n",
      "|    2|[1752, 1883, 1894...|[0.01487765424661...|\n",
      "|    3|[1254, 278, 590, ...|[0.01200005176723...|\n",
      "|    4|[29, 69, 66, 363,...|[0.01243888924043...|\n",
      "|    5|[14, 254, 248, 29...|[0.02229931830743...|\n",
      "|    6|[380, 662, 500, 6...|[0.00915894636570...|\n",
      "|    7|[0, 14, 52, 445, ...|[0.00791069041898...|\n",
      "|    8|[74, 177, 14, 242...|[0.00589018865339...|\n",
      "|    9|[725, 685, 1019, ...|[0.01814091710135...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DenseMatrix(8000, 10, [1.4247, 2.0198, 1.666, 1.2416, 4.0678, 5.435, 3.9712, 1.1773, ..., 2.9422, 0.59, 0.5267, 0.596, 0.6037, 0.467, 0.5371, 0.5065], 0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = model.describeTopics()\n",
    "topics.show()\n",
    "    \n",
    "model.topicsMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic:  0\n",
      "----------\n",
      "fals\n",
      "true\n",
      "adob\n",
      "dental\n",
      "temp\n",
      "wind\n",
      "rpa\n",
      "sg\n",
      "pdf\n",
      "janu\n",
      "----------\n",
      "topic:  1\n",
      "----------\n",
      "oxygen\n",
      "δp\n",
      "water\n",
      "hair\n",
      "permeat\n",
      "infant\n",
      "micro\n",
      "sperm\n",
      "hydrostat\n",
      "ga\n",
      "----------\n",
      "topic:  2\n",
      "----------\n",
      "noa\n",
      "micromesh\n",
      "protoplast\n",
      "inkjet\n",
      "text\n",
      "plant\n",
      "pnipaam\n",
      "silica\n",
      "adamski\n",
      "rgo\n",
      "----------\n",
      "topic:  3\n",
      "----------\n",
      "cilia\n",
      "nanoparticl\n",
      "nanochannel\n",
      "cell\n",
      "ip\n",
      "motil\n",
      "cytoplasm\n",
      "particl\n",
      "microtubul\n",
      "motor\n",
      "----------\n",
      "topic:  4\n",
      "----------\n",
      "particl\n",
      "dna\n",
      "electrod\n",
      "imped\n",
      "sensor\n",
      "sort\n",
      "frequenc\n",
      "electr\n",
      "acoust\n",
      "focus\n",
      "----------\n",
      "topic:  5\n",
      "----------\n",
      "droplet\n",
      "pcr\n",
      "bacteria\n",
      "digit\n",
      "dmf\n",
      "bacteri\n",
      "target\n",
      "polydispers\n",
      "rjp\n",
      "nucleic\n",
      "----------\n",
      "topic:  6\n",
      "----------\n",
      "crystal\n",
      "subdoc\n",
      "ctc\n",
      "dna\n",
      "lfa\n",
      "np\n",
      "cancer\n",
      "captur\n",
      "heat\n",
      "embryo\n",
      "----------\n",
      "topic:  7\n",
      "----------\n",
      "cell\n",
      "droplet\n",
      "cultur\n",
      "fiber\n",
      "hydrogel\n",
      "membran\n",
      "fig\n",
      "tissu\n",
      "spheroid\n",
      "lipid\n",
      "----------\n",
      "topic:  8\n",
      "----------\n",
      "blood\n",
      "magnet\n",
      "droplet\n",
      "antibodi\n",
      "dilut\n",
      "detect\n",
      "assay\n",
      "sampl\n",
      "plasma\n",
      "reagent\n",
      "----------\n",
      "topic:  9\n",
      "----------\n",
      "exosom\n",
      "vesicl\n",
      "atp\n",
      "ev\n",
      "zebrafish\n",
      "subtyp\n",
      "isol\n",
      "larva\n",
      "freestand\n",
      "guv\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "topics_rdd = topics.rdd\n",
    "\n",
    "topics_words = topics_rdd \\\n",
    "   .map(lambda row: row['termIndices']) \\\n",
    "   .map(lambda idx_list: [vocab[idx] for idx in idx_list]) \\\n",
    "   .collect()\n",
    "\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"topic: \", idx)\n",
    "    print(\"----------\")\n",
    "    for word in topic:\n",
    "        print(word)\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
