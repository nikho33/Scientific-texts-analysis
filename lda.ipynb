{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://PC-NLA:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Scientific papers analysis app</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x234ac298470>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local = \"local[*]\"\n",
    "\n",
    "appName = \"Scientific papers analysis app\"\n",
    "\n",
    "configLocale = SparkConf().setAppName(appName).setMaster(local). \\\n",
    "set(\"spark.executor.memory\", \"4G\"). \\\n",
    "set(\"spark.driver.memory\", \"4G\"). \\\n",
    "set(\"spark.sql.catalogImplementation\", \"in-memory\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=configLocale).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id :  local-1568364571280\n",
      "Version :  2.4.3\n"
     ]
    }
   ],
   "source": [
    "print(\"Id : \", sc.applicationId)\n",
    "print(\"Version : \", sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@connectwithghosh/topic-modelling-with-latent-dirichlet-allocation-lda-in-pyspark-2cb3ebd5678e\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For building the model\n",
    "from pyspark.ml.feature import CountVectorizer, HashingTF, IDF\n",
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "#from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.ml.clustering import LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------+----+--------------------+--------------------+--------------------+--------------------+\n",
      "|  filename|               title|      author|date|            abstract|            keywords|                text|          references|\n",
      "+----------+--------------------+------------+----+--------------------+--------------------+--------------------+--------------------+\n",
      "|PG0001.pdf|ai based personal...|chih-ming ho|2018|['when', 'cancer'...|['personalized', ...|['the', 'current'...|['jaynes', 'ding'...|\n",
      "+----------+--------------------+------------+----+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"./Data/DataMicroTAS/\"\n",
    "filename = \"Datas_MicroTAS2018.csv\"\n",
    "# Reading the data\n",
    "data = sqlContext.read.format(\"csv\")\\\n",
    "   .options(header='true', inferschema='true', delimiter='\\t') \\\n",
    "   .load(os.path.realpath(path + filename))\n",
    "\n",
    "#print(type(data), dir(data))\n",
    "data.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|  filename|                text|\n",
      "+----------+--------------------+\n",
      "|PG0001.pdf|[the, current, dr...|\n",
      "+----------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[filename: string, text: array<string>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selection = \"text\"\n",
    "data2 = data.select(\"filename\", F.regexp_replace(F.col(selection), \"[\\[\\]']\", \"\").alias(selection)) # [\\$#,]\n",
    "data2 = data2.select(\"filename\", split(col(selection), \",\\s*\").alias(selection))\n",
    "data2.show(1)\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokens = data.select(\"filename\", \"text\")\n",
    "#tokens.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. LDA algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|  filename|                text|\n",
      "+----------+--------------------+\n",
      "|PG0001.pdf|[the, current, dr...|\n",
      "|PG0004.pdf|[for, inertial, m...|\n",
      "+----------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[filename: string, text: array<string>]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_txts = sqlContext.createDataFrame(tokens, [\"list_of_words\",'index'])\n",
    "df_txts = data2.select(\"filename\", \"text\")\n",
    "df_txts.show(2)\n",
    "df_txts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        raw_features|\n",
      "+--------------------+\n",
      "|(8000,[0,1,2,3,4,...|\n",
      "|(8000,[0,1,2,3,4,...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/vsmolyakov/pyspark/blob/master/lda.py\n",
    "# TF\n",
    "num_features = 8000 \n",
    "cv = CountVectorizer(inputCol=\"text\", outputCol=\"raw_features\", vocabSize=num_features, minDF=2.0)\n",
    "cvmodel = cv.fit(df_txts)\n",
    "\n",
    "vocab = cvmodel.vocabulary\n",
    "\n",
    "result_cv = cvmodel.transform(df_txts)\n",
    "#result_cv = result_cv.drop('text')\n",
    "result_cv.select(\"raw_features\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hashting TF\n",
    "# TF: Both HashingTF and CountVectorizer can be used to generate the term frequency vectors.\n",
    "# HashingTF is a Transformer which takes sets of terms and converts those sets into fixed-length feature vectors.\n",
    "# https://spark.apache.org/docs/2.2.0/ml-features.html\n",
    "\n",
    "#hashingTF = HashingTF(inputCol=\"raw_features\", outputCol=\"tf_features\", numFeatures=num_features)\n",
    "#result_hashing = hashingTF.transform(result_cv)\n",
    "#result_hashing = newsgroups.drop('raw_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        raw_features|\n",
      "+--------------------+\n",
      "|(8000,[0,1,2,3,4,...|\n",
      "|(8000,[0,1,2,3,4,...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# IDF\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idfModel = idf.fit(result_cv)\n",
    "result_tfidf = idfModel.transform(result_cv) \n",
    "#result_tfidf = result_tfidf.drop('raw_features')\n",
    "result_cv.select(\"raw_features\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "num_topics = 12\n",
    "\n",
    "lda = LDA(k=num_topics, featuresCol=\"features\", seed=0)\n",
    "model = lda.fit(result_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|topic|         termIndices|         termWeights|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[1898, 3246, 3468...|[0.02274197800821...|\n",
      "|    1|[16, 22, 48, 69, ...|[0.00500314199675...|\n",
      "|    2|[492, 487, 443, 4...|[0.02390243273907...|\n",
      "|    3|[302, 1306, 721, ...|[0.00582053031667...|\n",
      "|    4|[234, 66, 146, 20...|[0.01161521049208...|\n",
      "|    5|[993, 1641, 148, ...|[0.00734704577482...|\n",
      "|    6|[48, 3473, 1598, ...|[0.00711728711204...|\n",
      "|    7|[72, 140, 213, 16...|[0.00805355519577...|\n",
      "|    8|[614, 775, 316, 1...|[0.00698549359581...|\n",
      "|    9|[1377, 1053, 1770...|[0.01041406570483...|\n",
      "|   10|[85, 236, 983, 40...|[0.02685210819708...|\n",
      "|   11|[145, 325, 54, 12...|[0.00351899485300...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DenseMatrix(8000, 12, [1.485, 1.4188, 1.3218, 1.0516, 1.2553, 0.7525, 1.0069, 1.4014, ..., 1.0809, 0.5618, 0.5973, 0.8915, 2.7706, 0.5982, 1.2325, 2.5687], 0)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/42258926/understanding-lda-in-spark\n",
    "# words by topics \n",
    "# https://stackoverflow.com/questions/53478934/pyspark-lda-get-words-in-topicss\n",
    "words_topic = 30\n",
    "topics = model.describeTopics(words_topic) \n",
    "topics.show()\n",
    "    \n",
    "model.topicsMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic:  0\n",
      "----------\n",
      "noa\n",
      "mbs\n",
      "gdna\n",
      "atp\n",
      "dibs\n",
      "mip\n",
      "rsd\n",
      "pla\n",
      "confinement\n",
      "mfps\n",
      "carrier\n",
      "pods\n",
      "reconfigurable\n",
      "dib\n",
      "nanocapillary\n",
      "icp\n",
      "microneedles\n",
      "proton\n",
      "mfp\n",
      "electrodeposition\n",
      "sticky\n",
      "receiver\n",
      "imposing\n",
      "probes\n",
      "sonication\n",
      "display\n",
      "streaming\n",
      "probe\n",
      "clustering\n",
      "ultrasound\n",
      "----------\n",
      "topic:  1\n",
      "----------\n",
      "cells\n",
      "cell\n",
      "droplet\n",
      "membrane\n",
      "droplets\n",
      "culture\n",
      "hydrogel\n",
      "fig\n",
      "lm\n",
      "printing\n",
      "tissue\n",
      "blood\n",
      "printed\n",
      "µm\n",
      "viability\n",
      "µl\n",
      "array\n",
      "the\n",
      "platform\n",
      "device\n",
      "agarose\n",
      "sample\n",
      "media\n",
      "chamber\n",
      "gel\n",
      "fbs\n",
      "liquid\n",
      "volume\n",
      "fiber\n",
      "lipid\n",
      "----------\n",
      "topic:  2\n",
      "----------\n",
      "imposing\n",
      "none\n",
      "quite\n",
      "plus\n",
      "normalise\n",
      "trim\n",
      "pddoc\n",
      "trimandshift\n",
      "option\n",
      "page\n",
      "agnws\n",
      "pages\n",
      "alldoc\n",
      "advanced\n",
      "shift\n",
      "points\n",
      "move\n",
      "fixed\n",
      "up\n",
      "subdoc\n",
      "biomaterial\n",
      "wheel\n",
      "range\n",
      "wound\n",
      "top\n",
      "stromal\n",
      "no\n",
      "both\n",
      "printhead\n",
      "epithelium\n",
      "----------\n",
      "topic:  3\n",
      "----------\n",
      "oxygen\n",
      "bbb\n",
      "silica\n",
      "valve\n",
      "endothelial\n",
      "fibers\n",
      "powder\n",
      "earthworm\n",
      "piezoelectric\n",
      "huvecs\n",
      "muscle\n",
      "igg\n",
      "parylene\n",
      "tile\n",
      "sheet\n",
      "building\n",
      "compound\n",
      "actuator\n",
      "gelma\n",
      "channel\n",
      "root\n",
      "micro\n",
      "construct\n",
      "force\n",
      "pdms\n",
      "bone\n",
      "teer\n",
      "elisa\n",
      "blocks\n",
      "oocytes\n",
      "----------\n",
      "topic:  4\n",
      "----------\n",
      "focusing\n",
      "particles\n",
      "particle\n",
      "magnetic\n",
      "beads\n",
      "separation\n",
      "flow\n",
      "inertial\n",
      "channel\n",
      "mixing\n",
      "droplet\n",
      "droplets\n",
      "streams\n",
      "aptamer\n",
      "position\n",
      "peo\n",
      "rate\n",
      "migration\n",
      "positions\n",
      "viscoelastic\n",
      "aptamers\n",
      "acoustic\n",
      "lateral\n",
      "field\n",
      "stream\n",
      "cilia\n",
      "spiral\n",
      "velocity\n",
      "wall\n",
      "microchannel\n",
      "----------\n",
      "topic:  5\n",
      "----------\n",
      "igg\n",
      "metering\n",
      "volume\n",
      "blood\n",
      "plasma\n",
      "oscillators\n",
      "paper\n",
      "constriction\n",
      "error\n",
      "t\n",
      "dbs\n",
      "µl\n",
      "drainage\n",
      "segmentation\n",
      "six\n",
      "hematocrit\n",
      "codes\n",
      "colon\n",
      "pva\n",
      "granulocytes\n",
      "pvdf\n",
      "sensor\n",
      "capillary\n",
      "input\n",
      "idt\n",
      "n\n",
      "filtration\n",
      "poct\n",
      "bioluminescence\n",
      "kda\n",
      "----------\n",
      "topic:  6\n",
      "----------\n",
      "droplet\n",
      "islets\n",
      "np\n",
      "gelatin\n",
      "microgels\n",
      "text\n",
      "nps\n",
      "droplets\n",
      "secretion\n",
      "wasting\n",
      "microdroplets\n",
      "polydopamine\n",
      "teflon\n",
      "stirring\n",
      "insulin\n",
      "monomer\n",
      "mitochondrial\n",
      "clearing\n",
      "hydrogel\n",
      "dopamine\n",
      "mf\n",
      "vi\n",
      "metabolic\n",
      "pnipaam\n",
      "microalgal\n",
      "accumulation\n",
      "pancreatic\n",
      "islet\n",
      "perfusable\n",
      "spheroids\n",
      "----------\n",
      "topic:  7\n",
      "----------\n",
      "blood\n",
      "cancer\n",
      "bacteria\n",
      "cells\n",
      "spheroids\n",
      "whole\n",
      "huvecs\n",
      "cell\n",
      "platelet\n",
      "mpa\n",
      "migration\n",
      "metastatic\n",
      "platelets\n",
      "leukocytes\n",
      "breast\n",
      "network\n",
      "repair\n",
      "wbcs\n",
      "leukocyte\n",
      "sand\n",
      "precipitation\n",
      "imaging\n",
      "tumor\n",
      "metabolites\n",
      "uptake\n",
      "treatment\n",
      "microfluidic\n",
      "chip\n",
      "chamber\n",
      "subpopulations\n",
      "----------\n",
      "topic:  8\n",
      "----------\n",
      "ctcs\n",
      "fibers\n",
      "sorting\n",
      "cells\n",
      "cell\n",
      "fluorescence\n",
      "drug\n",
      "mts\n",
      "membrane\n",
      "pvdf\n",
      "microcapsules\n",
      "hela\n",
      "imaging\n",
      "classification\n",
      "oil\n",
      "cytoplasm\n",
      "smart\n",
      "permeability\n",
      "stress\n",
      "morphological\n",
      "phase\n",
      "bbb\n",
      "waveform\n",
      "pathway\n",
      "spheroids\n",
      "monodisperse\n",
      "dld\n",
      "huvec\n",
      "deformability\n",
      "nucleus\n",
      "----------\n",
      "topic:  9\n",
      "----------\n",
      "neurons\n",
      "worms\n",
      "zebrafish\n",
      "electrotaxis\n",
      "worm\n",
      "elegans\n",
      "eggs\n",
      "embryos\n",
      "embryo\n",
      "larvae\n",
      "ips\n",
      "larval\n",
      "phenotyping\n",
      "ne\n",
      "neuron\n",
      "bm\n",
      "parasitic\n",
      "nfc\n",
      "hatching\n",
      "neuronal\n",
      "nematodes\n",
      "leukemic\n",
      "neural\n",
      "hypochlorite\n",
      "bosch\n",
      "aifa\n",
      "vancomycin\n",
      "parasite\n",
      "leukemia\n",
      "entrapment\n",
      "----------\n",
      "topic:  10\n",
      "----------\n",
      "dna\n",
      "pcr\n",
      "mirna\n",
      "amplification\n",
      "sequencing\n",
      "lamp\n",
      "hiv\n",
      "target\n",
      "strand\n",
      "primers\n",
      "hybridization\n",
      "nanopillar\n",
      "nucleic\n",
      "mirnas\n",
      "qpcr\n",
      "ssdna\n",
      "probe\n",
      "primer\n",
      "rna\n",
      "droplets\n",
      "genome\n",
      "electrophoresis\n",
      "isothermal\n",
      "photothermal\n",
      "elution\n",
      "amplified\n",
      "genomic\n",
      "template\n",
      "assay\n",
      "detection\n",
      "----------\n",
      "topic:  11\n",
      "----------\n",
      "sensor\n",
      "impedance\n",
      "detection\n",
      "nm\n",
      "electrode\n",
      "glucose\n",
      "electrodes\n",
      "particles\n",
      "surface\n",
      "nanoparticles\n",
      "fibrinogen\n",
      "signal\n",
      "phase\n",
      "electrical\n",
      "peak\n",
      "ph\n",
      "graphene\n",
      "ion\n",
      "gnps\n",
      "current\n",
      "glass\n",
      "electrochemical\n",
      "solution\n",
      "plasmonic\n",
      "microchannel\n",
      "value\n",
      "sensing\n",
      "spectra\n",
      "gold\n",
      "flow\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Visualizing topics with spark LDA\n",
    "# https://stackoverflow.com/questions/44233862/visualizing-topics-with-spark-lda\n",
    "\n",
    "topics_rdd = topics.rdd\n",
    "\n",
    "topics_words = topics_rdd \\\n",
    "   .map(lambda row: row['termIndices']) \\\n",
    "   .map(lambda idx_list: [vocab[idx] for idx in idx_list]) \\\n",
    "   .collect()\n",
    "\n",
    "topics_words_weights = topics_rdd \\\n",
    "   .map(lambda row: row['termWeights']) \\\n",
    "   .collect()\n",
    "\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"topic: \", idx)\n",
    "    print(\"----------\")\n",
    "    for word in topic:\n",
    "        print(word)\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['noa', 'mbs', 'gdna', 'atp', 'dibs', 'mip', 'rsd', 'pla', 'confinement', 'mfps', 'carrier', 'pods', 'reconfigurable', 'dib', 'nanocapillary', 'icp', 'microneedles', 'proton', 'mfp', 'electrodeposition', 'sticky', 'receiver', 'imposing', 'probes', 'sonication', 'display', 'streaming', 'probe', 'clustering', 'ultrasound']\n",
      "[22, 10, 10, 10, 6, 5, 5, 5, 5, 5, 4, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "#https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html\n",
    "#https://www.datacamp.com/community/tutorials/wordcloud-python\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from PIL import Image\n",
    "\n",
    "print(topics_words[0])\n",
    "my_rounded_list = [ math.floor(elem*1000) for elem in topics_words_weights[0]]\n",
    "print(my_rounded_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# https://stackoverflow.com/questions/43954114/python-wordcloud-repetitve-words\n",
    "\n",
    "def create_word_cloud(string):\n",
    "    maskArray = np.array(Image.open(\"cloud2.png\"))\n",
    "    #print(maskArray)\n",
    "    cloud = WordCloud(width = 800, height = 800,\n",
    "                      collocations=False,   # remove duplication of words that are too frequent\n",
    "                      background_color = \"white\", \n",
    "                      max_words = 200, mask = maskArray, \n",
    "                      stopwords = set(STOPWORDS))\n",
    "    cloud.generate(string)\n",
    "    cloud.to_file(\"WordCloud.png\")\n",
    "    \n",
    "def create_string(wordlist, wordweights):\n",
    "    i = 0\n",
    "    string_result = []\n",
    "    ampl_factor = 1000\n",
    "    \n",
    "    for word in wordlist:\n",
    "        for i in range(math.floor(wordweights[i]*ampl_factor)):\n",
    "            string_result.append(word)\n",
    "        i=+1\n",
    "    #print(string_result)\n",
    "    string_result = ' '.join(word for word in string_result)\n",
    "    return string_result\n",
    "    \n",
    "wordlist = topics_words[0][:]\n",
    "wordweights = topics_words_weights[0][:]\n",
    "    \n",
    "create_word_cloud(create_string(wordlist,wordweights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['graphene', 'plasmonic', 'confinement', 'rgo', 'mts', 'nanopillar', 'shift', 'sacrificial', 'microtube', 'dip'], ['cells', 'cell', 'culture', 'membrane', 'lm', 'tissue', 'bbb', 'printing', 'printed', 'viability'], ['imposing', 'none', 'plus', 'quite', 'normalise', 'pddoc', 'trim', 'trimandshift', 'option', 'page'], ['detection', 'surface', 'antibody', 'glass', 'valve', 'igg', 'digital', 'elisa', 'antibodies', 'silica'], ['focusing', 'particles', 'particle', 'magnetic', 'separation', 'flow', 'droplet', 'beads', 'channel', 'mixing'], ['gnps', 'metering', 'n', 'synthesis', 'volume', 'nanoparticles', 'biofilm', 'nanochannel', 'o', 'phases'], ['dna', 'droplet', 'droplets', 'pcr', 'mirna', 'amplification', 'sequencing', 'hiv', 'lamp', 'target'], ['blood', 'bacteria', 'cancer', 'huvecs', 'spheroids', 'whole', 'cells', 'platelet', 'mpa', 'sensor'], ['fibers', 'cilia', 'ctcs', 'cell', 'cells', 'sorting', 'pvdf', 'fluorescence', 'membrane', 'phase'], ['noa', 'embryo', 'neurons', 'zebrafish', 'bone', 'pnipaam', 'islets', 'electrotaxis', 'worms', 'elegans']]\n",
      "graphene plasmonic confinement rgo mts nanopillar shift sacrificial microtube dip dip dip dip dip dip\n"
     ]
    }
   ],
   "source": [
    "wordlist = topics_words[0][:]\n",
    "print(topics_words)\n",
    "for i in range(5):\n",
    "    wordlist.append('dip')\n",
    "\n",
    "string = ' '.join(word for word in wordlist)\n",
    "print(string)\n",
    "\n",
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "                    background_color ='white', \n",
    "                    stopwords = STOPWORDS, \n",
    "                    relative_scaling=1,\n",
    "                    min_font_size = 10).generate(string) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (4, 4), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
