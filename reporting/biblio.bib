@misc{Chang2015,
abstract = {A contextual analysis engine systematically extracts, ana lyzes and organizes digital content stored in an electronic file Such as a webpage. Content can be extracted using a text extraction module which is capable of separating the content which is to be analyzed from less meaningful content Such as format specifications and programming Scripts. The resulting unstructured corpus of plain text can then be passed to a text analytics module capable of generating a structured catego rization of topics included within the content. This structured categorization can be organized based on a content topic ontology which may have been previously defined or which may be developed in real-time. The systems disclosed herein optionally include an input/output interface capable of man aging workflows of the text extraction module and the text analytics module, administering a cache of previously gener ated results, and interfacing with otherapplications that lever age the disclosed contextual analysis services.},
author = {Chang, Walter and Chen, Chris and Sadler, Shone and Jared, David},
file = {:C$\backslash$:/Users/nicol/Documents/Python scripts/Scientific-texts-analysis/biblio/US20150106157A1{\_}Text Extraction Module for Contextual Analysis Engine.pdf:pdf},
title = {{Text extraction module for contextual analysis engine}},
year = {2015}
}
@inproceedings{,
booktitle = {The Semantic Web - ISWC 2008},
file = {:C$\backslash$:/Users/nicol/Documents/Python scripts/Scientific-texts-analysis/biblio/2008{\_}Conference Proceedings{\_}The-Semantic Web ISWC 2008.pdf:pdf},
title = {{Proceedings book}},
url = {http://www.mendeley.com/research/lecture-notes-computer-science-2/},
year = {2008}
}
@unpublished{Busemann1998,
author = {Busemann, Stephan},
file = {:C$\backslash$:/Users/nicol/Documents/Python scripts/Scientific-texts-analysis/biblio/1998{\_}Automated Text Summarization{\_}Busemann.pdf:pdf},
title = {{Automated Text Summarization}},
year = {1998}
}
@article{Vokey1999,
abstract = { Implicit knowledge is perhaps better understood as latent knowledge so that it is readily apparent that it contrasts with explicit knowledge in terms of the form of the knowledge representation, rather than by definition in terms of consciousness or awareness. We argue that as a practical matter any definition of the distinction between implicit and explicit knowledge further involves the notion of control. },
author = {Vokey, John R. and Higham, Philip A.},
doi = {10.1017/S0140525X99582186},
file = {:C$\backslash$:/Users/nicol/Documents/Python scripts/Scientific-texts-analysis/biblio/1999{\_}Implicit Knowledge as Automatic Latent Knowledge{\_}Vokey.pdf:pdf},
issn = {0140525X},
journal = {Behavioral and Brain Sciences},
number = {5},
pages = {787--788},
title = {{Implicit knowledge as automatic, latent knowledge}},
volume = {22},
year = {1999}
}
@article{Blei2010,
abstract = {We introduce supervised latent Dirichlet allocation (sLDA), a statistical model of labelled documents. The model accommodates a variety of response types. We derive an approximate maximum-likelihood procedure for parameter estimation, which relies on variational methods to handle intractable posterior expectations. Prediction problems motivate this research: we use the fitted model to predict response values for new documents. We test sLDA on two real-world problems: movie ratings predicted from reviews, and the political tone of amendments in the U.S. Senate based on the amendment text. We illustrate the benefits of sLDA versus modern regularized regression, as well as versus an unsupervised LDA analysis followed by a separate regression.},
archivePrefix = {arXiv},
arxivId = {arXiv:1003.0783v1},
author = {Blei, David M. and McAuliffe, Jon D.},
eprint = {arXiv:1003.0783v1},
file = {:C$\backslash$:/Users/nicol/Documents/Python scripts/Scientific-texts-analysis/biblio/2010{\_}Supervised topic models{\_}Blei.pdf:pdf},
isbn = {160560352X},
journal = {arXiv},
pages = {1--22},
title = {{Supervised topic models}},
year = {2010}
}
@article{Tshitoyan2019,
abstract = {The overwhelming majority of scientific knowledge is published as text, which is difficult to analyse by either traditional statistical analysis or modern machine learning methods. By contrast, the main source of machine-interpretable data for the materials research community has come from structured property databases1,2, which encompass only a small fraction of the knowledge present in the research literature. Beyond property values, publications contain valuable knowledge regarding the connections and relationships between data items as interpreted by the authors. To improve the identification and use of this knowledge, several studies have focused on the retrieval of information from scientific literature using supervised natural language processing3–10, which requires large hand-labelled datasets for training. Here we show that materials science knowledge present in the published literature can be efficiently encoded as information-dense word embeddings11–13 (vector representations of words) without human labelling or supervision. Without any explicit insertion of chemical knowledge, these embeddings capture complex materials science concepts such as the underlying structure of the periodic table and structure–property relationships in materials. Furthermore, we demonstrate that an unsupervised method can recommend materials for functional applications several years before their discovery. This suggests that latent knowledge regarding future discoveries is to a large extent embedded in past publications. Our findings highlight the possibility of extracting knowledge and relationships from the massive body of scientific literature in a collective manner, and point towards a generalized approach to the mining of scientific literature.},
author = {Tshitoyan, Vahe and Dagdelen, John and Weston, Leigh and Dunn, Alexander and Rong, Ziqin and Kononova, Olga and Persson, Kristin A. and Ceder, Gerbrand and Jain, Anubhav},
doi = {10.1038/s41586-019-1335-8},
file = {:C$\backslash$:/Users/nicol/Documents/Python scripts/Scientific-texts-analysis/biblio/2019{\_}Unsupervised word embeddings capture latent knowledge from materials science literature{\_}Tshitoyan.pdf:pdf},
issn = {14764687},
journal = {Nature},
number = {7763},
pages = {95--98},
publisher = {Springer US},
title = {{Unsupervised word embeddings capture latent knowledge from materials science literature}},
url = {http://dx.doi.org/10.1038/s41586-019-1335-8},
volume = {571},
year = {2019}
}
@article{Hashimi2015,
abstract = {Text mining techniques include categorization of text, summarization, topic detection, concept extraction, search and retrieval, document clustering, etc. Each of these techniques can be used in finding some non-trivial information from a collection of documents. Text mining can also be employed to detect a document's main topic/theme which is useful in creating taxonomy from the document collection. Areas of applications for text mining include publishing, media, telecommunications, marketing, research, healthcare, medicine, etc. Text mining has also been applied on many applications on the World Wide Web for developing recommendation systems. We propose here a set of criteria to evaluate the effectiveness of text mining techniques in an attempt to facilitate the selection of appropriate technique.},
author = {Hashimi, Hussein and Hafez, Alaaeldin and Mathkour, Hassan},
doi = {10.1016/j.chb.2014.10.062},
file = {:C$\backslash$:/Users/nicol/Documents/Python scripts/Scientific-texts-analysis/biblio/2015{\_}Selection criteria for text mining approaches{\_}Hashimi.pdf:pdf},
issn = {07475632},
journal = {Computers in Human Behavior},
keywords = {Classification,Clustering,Selection criteria,Text mining approaches},
pages = {729--733},
publisher = {Elsevier Ltd},
title = {{Selection criteria for text mining approaches}},
url = {http://dx.doi.org/10.1016/j.chb.2014.10.062},
volume = {51},
year = {2015}
}
@article{Allahyari2017,
abstract = {The amount of text that is generated every day is increasing dramatically. This tremendous volume of mostly unstructured text cannot be simply processed and perceived by computers. Therefore, efficient and effective techniques and algorithms are required to discover useful patterns. Text mining is the task of extracting meaningful information from text, which has gained significant attentions in recent years. In this paper, we describe several of the most fundamental text mining tasks and techniques including text pre-processing, classification and clustering. Additionally, we briefly explain text mining in biomedical and health care domains.},
archivePrefix = {arXiv},
arxivId = {1707.02919},
author = {Allahyari, Mehdi and Pouriyeh, Seyedamin and Assefi, Mehdi and Safaei, Saied and Trippe, Elizabeth D. and Gutierrez, Juan B. and Kochut, Krys},
eprint = {1707.02919},
file = {:C$\backslash$:/Users/nicol/Documents/Python scripts/Scientific-texts-analysis/biblio/2017{\_}A Brief Survey of Text Mining Classification, Clustering and Extraction Techniques{\_}Allahyari.pdf:pdf},
keywords = {classification,clustering,information extraction,information retrieval,text mining},
title = {{A Brief Survey of Text Mining: Classification, Clustering and Extraction Techniques}},
url = {http://arxiv.org/abs/1707.02919},
year = {2017}
}
@article{GurunathShivakumar2019,
abstract = {Word vector representations are a crucial part of natural language processing (NLP) and human computer interaction. In this paper, we propose a novel word vector representation, Confusion2Vec, motivated from the human speech production and perception that encodes representational ambiguity. Humans employ both acoustic similarity cues and contextual cues to decode information and we focus on a model that incorporates both sources of information. The representational ambiguity of acoustics, which manifests itself in word confusions, is often resolved by both humans and machines through contextual cues. A range of representational ambiguities can emerge in various domains further to acoustic perception, such as morphological transformations, word segmentation, paraphrasing for NLP tasks like machine translation, etc. In this work, we present a case study in application to automatic speech recognition (ASR) task, where the word representational ambiguities/confusions are related to acoustic similarity. We present several techniques to train an acoustic perceptual similarity representation ambiguity. We term this Confusion2Vec and learn on unsupervised-generated data from ASR confusion networks or lattice-like structures. Appropriate evaluations for the Confusion2Vec are formulated for gauging acoustic similarity in addition to semantic–syntactic and word similarity evaluations. The Confusion2Vec is able to model word confusions efficiently, without compromising on the semantic-syntactic word relations, thus effectively enriching the word vector space with extra task relevant ambiguity information. We provide an intuitive exploration of the two-dimensional Confusion2Vec space using principal component analysis of the embedding and relate to semantic relationships, syntactic relationships, and acoustic relationships. We show through this that the new space preserves the semantic/syntactic relationships while robustly encoding acoustic similarities. The potential of the new vector representation and its ability in the utilization of uncertainty information associated with the lattice is demonstrated through small examples relating to the task of ASR error correction.},
archivePrefix = {arXiv},
arxivId = {arXiv:1811.03199v2},
author = {{Gurunath Shivakumar}, Prashanth and Georgiou, Panayiotis},
doi = {10.7717/peerj-cs.195},
eprint = {arXiv:1811.03199v2},
file = {:C$\backslash$:/Users/nicol/Documents/Python scripts/Scientific-texts-analysis/biblio/2019{\_}Confusion2Vec Towards Enriching Vector Space Word Representations with Representational Ambiguities{\_}Shivakumar.pdf:pdf},
journal = {PeerJ Computer Science},
pages = {e195},
title = {{Confusion2Vec: towards enriching vector space word representations with representational ambiguities}},
volume = {5},
year = {2019}
}
@phdthesis{Ghannay2018,
author = {Ghannay, Sahar},
file = {:C$\backslash$:/Users/nicol/Documents/Python scripts/Scientific-texts-analysis/biblio/2017{\_}Etude sur les representations continues de mots appliquees a la detection automatique des erreurs de la reconnaissance de la parole{\_}Thesis Ghannay.pdf:pdf},
school = {Universit{\'{e}} Bretagne Loire/Le Mans Universit{\'{e}}},
title = {{Etude sur les representations continues de mots appliquees {\`{a}} la detection automatique des erreurs de reconnaissance de la parole}},
year = {2018}
}
@article{Wang2018,
abstract = {Automobile insurance fraud represents a pivotal percentage of property insurance companies' costs and affects the companies' pricing strategies and social economic benefits in the long term. Automobile insurance fraud detection has become critically important for reducing the costs of insurance companies. Previous studies on automobile insurance fraud detection examined various numeric factors, such as the time of the claim and the brand of the insured car. However, the textual information in the claims has rarely been studied to analyze insurance fraud. This paper proposes a novel deep learning model for automobile insurance fraud detection that uses Latent Dirichlet Allocation (LDA)-based text analytics. In our proposed method, LDA is first used to extract the text features hiding in the text descriptions of the accidents appearing in the claims, and deep neural networks then are trained on the data, which include the text features and traditional numeric features for detecting fraudulent claims. Based on the real-world insurance fraud dataset, our experimental results reveal that the proposed text analytics-based framework outperforms a traditional one. Furthermore, the experimental results show that the deep neural networks outperform widely used machine learning models, such as random forests and support vector machine. Therefore, our proposed framework that combines deep neural networks and LDA is a suitable potential tool for automobile insurance fraud detection.},
author = {Wang, Yibo and Xu, Wei},
doi = {10.1016/j.dss.2017.11.001},
file = {:C$\backslash$:/Users/nicol/Documents/Python scripts/Scientific-texts-analysis/biblio/2018{\_}Leveraging deep learning with LDA-based text analytics to detect automobile insurance fraud{\_}Wang.pdf:pdf},
issn = {01679236},
journal = {Decision Support Systems},
keywords = {Deep learning,Fraud detection,Insurance fraud,Text analytics,Topic modeling},
pages = {87--95},
publisher = {Elsevier B.V},
title = {{Leveraging deep learning with LDA-based text analytics to detect automobile insurance fraud}},
url = {http://dx.doi.org/10.1016/j.dss.2017.11.001},
volume = {105},
year = {2018}
}
@article{Blei2003,
author = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
file = {:C$\backslash$:/Users/nicol/Documents/Python scripts/Scientific-texts-analysis/biblio/2003{\_}Latent Dirichlet Allocation{\_}Blei.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {993--1022},
title = {{Latent Dirichlet Allocation}},
url = {http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf},
volume = {3},
year = {2003}
}
@inproceedings{Blei2007,
abstract = {We introduce supervised latent Dirichlet allocation (sLDA), a statistical model of labelled documents. The model accommodates a variety of response types. We derive a maximum-likelihood procedure for parameter estimation, which relies on variational approximations to handle intractable posterior expectations. Prediction problems motivate this research: we use the fitted model to predict response values for new documents. We test sLDA on two real-world problems: movie ratings predicted from reviews, and web page popularity predicted from text descriptions. We illustrate the benefits of sLDA versus modern regularized regression, as well as versus an unsupervised LDA analysis followed by a separate regression.},
author = {Blei, David M. and McAuliffe, Jon D.},
booktitle = {Advances in Neural Information Processing Systems 20 (NIPS 2007)},
file = {:C$\backslash$:/Users/nicol/Documents/Python scripts/Scientific-texts-analysis/biblio/2008{\_}Supervised topic models{\_}Blei.pdf:pdf},
pages = {1--8},
title = {{Supervised topic models}},
year = {2007}
}
@inproceedings{Allahyari2016,
abstract = {17th International Conference Shanghai, China, November 8 – 10, 2016 Proceedings, Part II},
author = {Allahyari, Mehdi and Kochut, Krys},
booktitle = {International Conference on Web Information Systems Engineering},
doi = {10.1007/978-3-319-48740-3},
file = {:C$\backslash$:/Users/nicol/Documents/Python scripts/Scientific-texts-analysis/biblio/2016{\_}Semantic Context-aware Recommendation via Topic Models Leveraging Linked Open Data{\_}Allahyari.pdf:pdf},
isbn = {9783319487397},
pages = {263--277},
title = {{Semantic Context-aware Recommendation via Topic Models Leveraging Linked Open Data}},
year = {2016}
}
@inproceedings{AlSumait2008,
abstract = {This paper presents Online TopicModel (OLDA), a topic model that automatically captures the thematic patterns and identifies emerging topics of text streams and their changes over time. Our approach allows the topic modeling framework, specifically the Latent Dirichlet Allocation (LDA) model, to work in an online fashion such that it incrementally builds an up-to-date model (mixture of topics per document and mixture of words per topic) when a new document (or a set of documents) appears. A solution based on the Empirical Bayes method is proposed. The idea is to incrementally update the current model according to the information inferred from the new stream of data with no need to access previous data. The dynamics of the proposed approach also provide an efficient mean to track the topics over time and detect the emerging topics in real time. Our method is evaluated both qualitatively and quantitatively using benchmark datasets. In our experiments, the OLDA has discovered interesting patterns by just analyzing a fraction of data at a time. Our tests also prove the ability of OLDA to align the topics across the epochs with which the evolution of the topics over time is captured. The OLDA is also comparable to, and sometimes better than, the original LDA in predicting the likelihood of unseen documents.},
author = {AlSumait, Loulwah and Barbara, Daniel and Domeniconi, Carlotta},
booktitle = {Proceedings - IEEE International Conference on Data Mining, ICDM},
doi = {10.1109/ICDM.2008.140},
file = {:C$\backslash$:/Users/nicol/Documents/Python scripts/Scientific-texts-analysis/biblio/2008{\_}On-line LDA Adaptive Topic Models for Mining Text Streams with Applications to Topic Detection and Tracking{\_}AlSumait.pdf:pdf},
isbn = {9780769535029},
issn = {15504786},
pages = {3--12},
title = {{On-line LDA: Adaptive topic models for mining text streams with applications to topic detection and tracking}},
year = {2008}
}
@article{Nasukawa2009,
abstract = {Large text databases potentially contain a great wealth of knowledge. However, text represents factual information (and information about the author's communicative intentions) in a complex, rich, and opaque manner. Consequently, unlike numerical and fixed field data, it cannot be analyzed by standard statistical data mining methods. Relying on human analysis results in either huge workloads or the analysis of only a tiny fraction of the database. We are working on text mining technology to extract knowledge from very large amounts of textual data. Unlike information retrieval technology that allows a user to select documents that meet the user's requirements and interests, or document clustering technology that organizes documents, we focus on finding valuable patterns and rules in text that indicate trends and significant features about specific topics. By applying our prototype system named TAKMI (Text Analysis and Knowledge MIning) to textual databases in PC help centers, we can automatically detect product failures; determine issues that have led to rapid increases in the number of calls and their underlying reasons; and analyze help center productivity and changes in customers' behavior involving a particular product, without reading any of the text. We have verified that our framework is also effective for other data such as patent documents.},
author = {Nasukawa, Tetsuya and Nagano, Tohru},
file = {:C$\backslash$:/Users/nicol/Documents/Python scripts/Scientific-texts-analysis/biblio/2001{\_}Text analysis and knowledge mining system{\_}Nasukawa.pdf:pdf},
journal = {IBM Systems Journal},
number = {4},
pages = {967--984},
title = {{Text analysis and knowledge mining}},
volume = {40},
year = {2001}
}
@inproceedings{Allahyari2016a,
abstract = {Probabilistic topic models are powerful techniques which are widely used for discovering topics or semantic content from a large collection of documents. However, because topic models are entirely unsupervised, they may lead to topics that are not understandable in applications. Recently, several knowledge-based topic models have been proposed which primarily use word-level domain knowledge in the model to enhance the topic coherence and ignore the rich information carried by entities (e.g persons, location, organizations, etc.) associated with the documents. Additionally, there exists a vast amount of prior knowledge (background knowledge) represented as ontologies and Linked Open Data (LOD), which can be incorporated into the topic models to produce coherent topics. In this paper, we introduce a novel entity-based topic model, called EntLDA, to effectively integrate an ontology with an entity topic model to improve the topic modeling process. Furthermore, to increase the coherence of the identified topics, we introduce a novel ontology-based regularization framework, which is then integrated with the EntLDA model. Our experimental results demonstrate the effectiveness of the proposed model in improving the coherence of the topics.},
author = {Allahyari, Mehdi and Kochut, Krys},
booktitle = {Proceedings - 2016 IEEE/WIC/ACM International Conference on Web Intelligence, WI 2016},
doi = {10.1109/WI.2016.0015},
file = {:C$\backslash$:/Users/nicol/Documents/Python scripts/Scientific-texts-analysis/biblio/2016{\_}Discovering Coherent Topics with Entity Topic Models{\_}Allahyari.pdf:pdf},
isbn = {9781509044702},
keywords = {Ontologies,Semantic Web,Statistical learning,Topic coherence,Topic modeling},
pages = {26--33},
title = {{Discovering Coherent Topics with Entity Topic Models}},
year = {2016}
}
@article{Hoffman2010,
abstract = {We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by fitting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA finds topic models as good or better than those found with batch VB, and in a fraction of the time.},
archivePrefix = {arXiv},
arxivId = {0710.4428v1},
author = {Hoffman, Matthew D and Blei, David M and Bach, Francis},
doi = {10.1.1.187.1883},
eprint = {0710.4428v1},
file = {:C$\backslash$:/Users/nicol/Documents/Python scripts/Scientific-texts-analysis/biblio/2010{\_}Online Learning for Latent Dirichlet Allocation{\_}Hoffman.pdf:pdf},
isbn = {9781450300551},
issn = {08912017},
journal = {Neural Information Processing Systems Conference (NIPS 2010)},
pages = {1--9},
pmid = {4944952},
title = {{Online Learning for Latent Dirichlet Allocation}},
url = {http://papers.nips.cc/paper/3902-online-learning-for-latent-dirichlet-allocation.pdf},
year = {2010}
}
